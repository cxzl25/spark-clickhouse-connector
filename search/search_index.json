{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview Spark ClickHouse Connector is a high performance connector build on top of Spark DataSource V2. Requirements Basic knowledge of Apache Spark and ClickHouse . An available ClickHouse single node or cluster. An available Spark cluster, and Spark version should be 3.3, because we need the interfaces of Spark DataSource V2 added in 3.3.0. Make sure your network policy satisfies the following requirements, both driver and executor of Spark need to access ClickHouse HTTP/gRPC port. If you are using it to access ClickHouse cluster, ensure the connectivity between driver and executor of Spark and each node of ClickHouse cluster. Notes Integration tests based on Java 8&11, Scala 2.12&2.13, Spark 3.3.0 and ClickHouse v22.3.3.44, with both single ClickHouse instance and ClickHouse cluster.","title":"Home"},{"location":"#overview","text":"Spark ClickHouse Connector is a high performance connector build on top of Spark DataSource V2.","title":"Overview"},{"location":"#requirements","text":"Basic knowledge of Apache Spark and ClickHouse . An available ClickHouse single node or cluster. An available Spark cluster, and Spark version should be 3.3, because we need the interfaces of Spark DataSource V2 added in 3.3.0. Make sure your network policy satisfies the following requirements, both driver and executor of Spark need to access ClickHouse HTTP/gRPC port. If you are using it to access ClickHouse cluster, ensure the connectivity between driver and executor of Spark and each node of ClickHouse cluster.","title":"Requirements"},{"location":"#notes","text":"Integration tests based on Java 8&11, Scala 2.12&2.13, Spark 3.3.0 and ClickHouse v22.3.3.44, with both single ClickHouse instance and ClickHouse cluster.","title":"Notes"},{"location":"best_practices/","text":"TODO","title":"Index"},{"location":"best_practices/#todo","text":"","title":"TODO"},{"location":"best_practices/01_deployment/","text":"Deployment Jar Put clickhouse-spark-runtime-3.3_2.12-0.5.0.jar and clickhouse-jdbc-0.3.2-patch11-all.jar into $SPARK_HOME/jars/ , then you don't need to bundle the jar into your Spark application, and --jar is not required when using spark-shell or spark-sql (again, for SQL-only use cases, Apache Kyuubi is recommended for Production). Configuration Persist catalog configurations into $SPARK_HOME/conf/spark-defaults.conf , then --conf s are not required when using spark-shell or spark-sql . spark.sql.catalog.ck_01=xenon.clickhouse.ClickHouseCatalog spark.sql.catalog.ck_01.host=10.0.0.1 spark.sql.catalog.ck_01.protocol=http spark.sql.catalog.ck_01.http_port=8123 spark.sql.catalog.ck_01.user=app spark.sql.catalog.ck_01.password=pwd spark.sql.catalog.ck_01.database=default spark.sql.catalog.ck_02=xenon.clickhouse.ClickHouseCatalog spark.sql.catalog.ck_02.host=10.0.0.2 spark.sql.catalog.ck_02.protocol=http spark.sql.catalog.ck_02.http_port=8123 spark.sql.catalog.ck_02.user=app spark.sql.catalog.ck_02.password=pwd spark.sql.catalog.ck_02.database=default","title":"Deployment"},{"location":"best_practices/01_deployment/#deployment","text":"","title":"Deployment"},{"location":"best_practices/01_deployment/#jar","text":"Put clickhouse-spark-runtime-3.3_2.12-0.5.0.jar and clickhouse-jdbc-0.3.2-patch11-all.jar into $SPARK_HOME/jars/ , then you don't need to bundle the jar into your Spark application, and --jar is not required when using spark-shell or spark-sql (again, for SQL-only use cases, Apache Kyuubi is recommended for Production).","title":"Jar"},{"location":"best_practices/01_deployment/#configuration","text":"Persist catalog configurations into $SPARK_HOME/conf/spark-defaults.conf , then --conf s are not required when using spark-shell or spark-sql . spark.sql.catalog.ck_01=xenon.clickhouse.ClickHouseCatalog spark.sql.catalog.ck_01.host=10.0.0.1 spark.sql.catalog.ck_01.protocol=http spark.sql.catalog.ck_01.http_port=8123 spark.sql.catalog.ck_01.user=app spark.sql.catalog.ck_01.password=pwd spark.sql.catalog.ck_01.database=default spark.sql.catalog.ck_02=xenon.clickhouse.ClickHouseCatalog spark.sql.catalog.ck_02.host=10.0.0.2 spark.sql.catalog.ck_02.protocol=http spark.sql.catalog.ck_02.http_port=8123 spark.sql.catalog.ck_02.user=app spark.sql.catalog.ck_02.password=pwd spark.sql.catalog.ck_02.database=default","title":"Configuration"},{"location":"configurations/","text":"Configurations Catalog Configurations Single Instance Suppose you have one ClickHouse instance which installed on 10.0.0.1 and exposes HTTP on 8123 . Edit $SPARK_HOME/conf/spark-defaults.conf . # register a catalog named \"clickhouse\" spark.sql.catalog.clickhouse xenon.clickhouse.ClickHouseCatalog # basic configurations for \"clickhouse\" catalog spark.sql.catalog.clickhouse.host 10.0.0.1 spark.sql.catalog.clickhouse.protocol http spark.sql.catalog.clickhouse.http_port 8123 spark.sql.catalog.clickhouse.user default spark.sql.catalog.clickhouse.password spark.sql.catalog.clickhouse.database default # custom options of clickhouse-client for \"clickhouse\" catalog spark.sql.catalog.clickhouse.option.async false spark.sql.catalog.clickhouse.option.client_name spark Then you can access ClickHouse table <ck_db>.<ck_table> from Spark SQL by using clickhouse.<ck_db>.<ck_table> . Cluster For ClickHouse cluster, give an unique catalog name for each instances. Suppose you have two ClickHouse instances, one installed on 10.0.0.1 and exposes gRPC on port 9100 named clickhouse1, and another installed on 10.0.0.2 and exposes gRPC on port 9100 named clickhouse2. Edit $SPARK_HOME/conf/spark-defaults.conf . spark.sql.catalog.clickhouse1 xenon.clickhouse.ClickHouseCatalog spark.sql.catalog.clickhouse1.host 10.0.0.1 spark.sql.catalog.clickhouse1.protocol grpc spark.sql.catalog.clickhouse1.grpc_port 9100 spark.sql.catalog.clickhouse1.user default spark.sql.catalog.clickhouse1.password spark.sql.catalog.clickhouse1.database default spark.sql.catalog.clickhouse1.option.async false spark.sql.catalog.clickhouse2 xenon.clickhouse.ClickHouseCatalog spark.sql.catalog.clickhouse2.host 10.0.0.2 spark.sql.catalog.clickhouse2.protocol grpc spark.sql.catalog.clickhouse2.grpc_port 9100 spark.sql.catalog.clickhouse2.user default spark.sql.catalog.clickhouse2.password spark.sql.catalog.clickhouse2.database default spark.sql.catalog.clickhouse2.option.async false Then you can access clickhouse1 table <ck_db>.<ck_table> from Spark SQL by clickhouse1.<ck_db>.<ck_table> , and access clickhouse2 table <ck_db>.<ck_table> by clickhouse2.<ck_db>.<ck_table> . SQL Configurations SQL Configurations could be overwritten by SET <key>=<value> in runtime. Key Default Description Since spark.clickhouse.ignoreUnsupportedTransform false ClickHouse supports using complex expressions as sharding keys or partition values, e.g. cityHash64(col_1, col_2) , and those can not be supported by Spark now. If true , ignore the unsupported expressions, otherwise fail fast w/ an exception. Note: when spark.clickhouse.write.distributed.convertLocal is enabled, ignore unsupported sharding keys may corrupt the data. 0.4.0 spark.clickhouse.read.compression.codec lz4 The codec used to decompress data for reading. Supported codecs: none, lz4. 0.5.0 spark.clickhouse.read.distributed.convertLocal true When reading Distributed table, read local table instead of itself. If true , ignore spark.clickhouse.read.distributed.useClusterNodes . 0.1.0 spark.clickhouse.read.format json Serialize format for reading. Supported formats: json, binary 0.6.0 spark.clickhouse.read.splitByPartitionId true If true , construct input partition filter by virtual column _partition_id , instead of partition value. There are known bugs to assemble SQL predication by partition value. This feature requires ClickHouse Server v21.6+ 0.4.0 spark.clickhouse.write.batchSize 10000 The number of records per batch on writing to ClickHouse. 0.1.0 spark.clickhouse.write.compression.codec lz4 The codec used to compress data for writing. Supported codecs: none, lz4. 0.3.0 spark.clickhouse.write.distributed.convertLocal false When writing Distributed table, write local table instead of itself. If true , ignore spark.clickhouse.write.distributed.useClusterNodes . 0.1.0 spark.clickhouse.write.distributed.useClusterNodes true Write to all nodes of cluster when writing Distributed table. 0.1.0 spark.clickhouse.write.format arrow Serialize format for writing. Supported formats: json, arrow 0.4.0 spark.clickhouse.write.localSortByKey true If true , do local sort by sort keys before writing. 0.3.0 spark.clickhouse.write.localSortByPartition If true , do local sort by partition before writing. If not set, it equals to spark.clickhouse.write.repartitionByPartition . 0.3.0 spark.clickhouse.write.maxRetry 3 The maximum number of write we will retry for a single batch write failed with retryable codes. 0.1.0 spark.clickhouse.write.repartitionByPartition true Whether to repartition data by ClickHouse partition keys to meet the distributions of ClickHouse table before writing. 0.3.0 spark.clickhouse.write.repartitionNum 0 Repartition data to meet the distributions of ClickHouse table is required before writing, use this conf to specific the repartition number, value less than 1 mean no requirement. 0.1.0 spark.clickhouse.write.repartitionStrictly false If true , Spark will strictly distribute incoming records across partitions to satisfy the required distribution before passing the records to the data source table on write. Otherwise, Spark may apply certain optimizations to speed up the query but break the distribution requirement. Note, this configuration requires SPARK-37523, w/o this patch, it always act as true . 0.3.0 spark.clickhouse.write.retryInterval 10s The interval in seconds between write retry. 0.1.0 spark.clickhouse.write.retryableErrorCodes 241 The retryable error codes returned by ClickHouse server when write failing. 0.1.0","title":"Index"},{"location":"configurations/#configurations","text":"","title":"Configurations"},{"location":"configurations/#catalog-configurations","text":"","title":"Catalog Configurations"},{"location":"configurations/#single-instance","text":"Suppose you have one ClickHouse instance which installed on 10.0.0.1 and exposes HTTP on 8123 . Edit $SPARK_HOME/conf/spark-defaults.conf . # register a catalog named \"clickhouse\" spark.sql.catalog.clickhouse xenon.clickhouse.ClickHouseCatalog # basic configurations for \"clickhouse\" catalog spark.sql.catalog.clickhouse.host 10.0.0.1 spark.sql.catalog.clickhouse.protocol http spark.sql.catalog.clickhouse.http_port 8123 spark.sql.catalog.clickhouse.user default spark.sql.catalog.clickhouse.password spark.sql.catalog.clickhouse.database default # custom options of clickhouse-client for \"clickhouse\" catalog spark.sql.catalog.clickhouse.option.async false spark.sql.catalog.clickhouse.option.client_name spark Then you can access ClickHouse table <ck_db>.<ck_table> from Spark SQL by using clickhouse.<ck_db>.<ck_table> .","title":"Single Instance"},{"location":"configurations/#cluster","text":"For ClickHouse cluster, give an unique catalog name for each instances. Suppose you have two ClickHouse instances, one installed on 10.0.0.1 and exposes gRPC on port 9100 named clickhouse1, and another installed on 10.0.0.2 and exposes gRPC on port 9100 named clickhouse2. Edit $SPARK_HOME/conf/spark-defaults.conf . spark.sql.catalog.clickhouse1 xenon.clickhouse.ClickHouseCatalog spark.sql.catalog.clickhouse1.host 10.0.0.1 spark.sql.catalog.clickhouse1.protocol grpc spark.sql.catalog.clickhouse1.grpc_port 9100 spark.sql.catalog.clickhouse1.user default spark.sql.catalog.clickhouse1.password spark.sql.catalog.clickhouse1.database default spark.sql.catalog.clickhouse1.option.async false spark.sql.catalog.clickhouse2 xenon.clickhouse.ClickHouseCatalog spark.sql.catalog.clickhouse2.host 10.0.0.2 spark.sql.catalog.clickhouse2.protocol grpc spark.sql.catalog.clickhouse2.grpc_port 9100 spark.sql.catalog.clickhouse2.user default spark.sql.catalog.clickhouse2.password spark.sql.catalog.clickhouse2.database default spark.sql.catalog.clickhouse2.option.async false Then you can access clickhouse1 table <ck_db>.<ck_table> from Spark SQL by clickhouse1.<ck_db>.<ck_table> , and access clickhouse2 table <ck_db>.<ck_table> by clickhouse2.<ck_db>.<ck_table> .","title":"Cluster"},{"location":"configurations/#sql-configurations","text":"SQL Configurations could be overwritten by SET <key>=<value> in runtime. Key Default Description Since spark.clickhouse.ignoreUnsupportedTransform false ClickHouse supports using complex expressions as sharding keys or partition values, e.g. cityHash64(col_1, col_2) , and those can not be supported by Spark now. If true , ignore the unsupported expressions, otherwise fail fast w/ an exception. Note: when spark.clickhouse.write.distributed.convertLocal is enabled, ignore unsupported sharding keys may corrupt the data. 0.4.0 spark.clickhouse.read.compression.codec lz4 The codec used to decompress data for reading. Supported codecs: none, lz4. 0.5.0 spark.clickhouse.read.distributed.convertLocal true When reading Distributed table, read local table instead of itself. If true , ignore spark.clickhouse.read.distributed.useClusterNodes . 0.1.0 spark.clickhouse.read.format json Serialize format for reading. Supported formats: json, binary 0.6.0 spark.clickhouse.read.splitByPartitionId true If true , construct input partition filter by virtual column _partition_id , instead of partition value. There are known bugs to assemble SQL predication by partition value. This feature requires ClickHouse Server v21.6+ 0.4.0 spark.clickhouse.write.batchSize 10000 The number of records per batch on writing to ClickHouse. 0.1.0 spark.clickhouse.write.compression.codec lz4 The codec used to compress data for writing. Supported codecs: none, lz4. 0.3.0 spark.clickhouse.write.distributed.convertLocal false When writing Distributed table, write local table instead of itself. If true , ignore spark.clickhouse.write.distributed.useClusterNodes . 0.1.0 spark.clickhouse.write.distributed.useClusterNodes true Write to all nodes of cluster when writing Distributed table. 0.1.0 spark.clickhouse.write.format arrow Serialize format for writing. Supported formats: json, arrow 0.4.0 spark.clickhouse.write.localSortByKey true If true , do local sort by sort keys before writing. 0.3.0 spark.clickhouse.write.localSortByPartition If true , do local sort by partition before writing. If not set, it equals to spark.clickhouse.write.repartitionByPartition . 0.3.0 spark.clickhouse.write.maxRetry 3 The maximum number of write we will retry for a single batch write failed with retryable codes. 0.1.0 spark.clickhouse.write.repartitionByPartition true Whether to repartition data by ClickHouse partition keys to meet the distributions of ClickHouse table before writing. 0.3.0 spark.clickhouse.write.repartitionNum 0 Repartition data to meet the distributions of ClickHouse table is required before writing, use this conf to specific the repartition number, value less than 1 mean no requirement. 0.1.0 spark.clickhouse.write.repartitionStrictly false If true , Spark will strictly distribute incoming records across partitions to satisfy the required distribution before passing the records to the data source table on write. Otherwise, Spark may apply certain optimizations to speed up the query but break the distribution requirement. Note, this configuration requires SPARK-37523, w/o this patch, it always act as true . 0.3.0 spark.clickhouse.write.retryInterval 10s The interval in seconds between write retry. 0.1.0 spark.clickhouse.write.retryableErrorCodes 241 The retryable error codes returned by ClickHouse server when write failing. 0.1.0","title":"SQL Configurations"},{"location":"configurations/01_catalog_configurations/","text":"Single Instance Suppose you have one ClickHouse instance which installed on 10.0.0.1 and exposes HTTP on 8123 . Edit $SPARK_HOME/conf/spark-defaults.conf . # register a catalog named \"clickhouse\" spark.sql.catalog.clickhouse xenon.clickhouse.ClickHouseCatalog # basic configurations for \"clickhouse\" catalog spark.sql.catalog.clickhouse.host 10.0.0.1 spark.sql.catalog.clickhouse.protocol http spark.sql.catalog.clickhouse.http_port 8123 spark.sql.catalog.clickhouse.user default spark.sql.catalog.clickhouse.password spark.sql.catalog.clickhouse.database default # custom options of clickhouse-client for \"clickhouse\" catalog spark.sql.catalog.clickhouse.option.async false spark.sql.catalog.clickhouse.option.client_name spark Then you can access ClickHouse table <ck_db>.<ck_table> from Spark SQL by using clickhouse.<ck_db>.<ck_table> . Cluster For ClickHouse cluster, give an unique catalog name for each instances. Suppose you have two ClickHouse instances, one installed on 10.0.0.1 and exposes gRPC on port 9100 named clickhouse1, and another installed on 10.0.0.2 and exposes gRPC on port 9100 named clickhouse2. Edit $SPARK_HOME/conf/spark-defaults.conf . spark.sql.catalog.clickhouse1 xenon.clickhouse.ClickHouseCatalog spark.sql.catalog.clickhouse1.host 10.0.0.1 spark.sql.catalog.clickhouse1.protocol grpc spark.sql.catalog.clickhouse1.grpc_port 9100 spark.sql.catalog.clickhouse1.user default spark.sql.catalog.clickhouse1.password spark.sql.catalog.clickhouse1.database default spark.sql.catalog.clickhouse1.option.async false spark.sql.catalog.clickhouse2 xenon.clickhouse.ClickHouseCatalog spark.sql.catalog.clickhouse2.host 10.0.0.2 spark.sql.catalog.clickhouse2.protocol grpc spark.sql.catalog.clickhouse2.grpc_port 9100 spark.sql.catalog.clickhouse2.user default spark.sql.catalog.clickhouse2.password spark.sql.catalog.clickhouse2.database default spark.sql.catalog.clickhouse2.option.async false Then you can access clickhouse1 table <ck_db>.<ck_table> from Spark SQL by clickhouse1.<ck_db>.<ck_table> , and access clickhouse2 table <ck_db>.<ck_table> by clickhouse2.<ck_db>.<ck_table> .","title":"01 catalog configurations"},{"location":"configurations/01_catalog_configurations/#single-instance","text":"Suppose you have one ClickHouse instance which installed on 10.0.0.1 and exposes HTTP on 8123 . Edit $SPARK_HOME/conf/spark-defaults.conf . # register a catalog named \"clickhouse\" spark.sql.catalog.clickhouse xenon.clickhouse.ClickHouseCatalog # basic configurations for \"clickhouse\" catalog spark.sql.catalog.clickhouse.host 10.0.0.1 spark.sql.catalog.clickhouse.protocol http spark.sql.catalog.clickhouse.http_port 8123 spark.sql.catalog.clickhouse.user default spark.sql.catalog.clickhouse.password spark.sql.catalog.clickhouse.database default # custom options of clickhouse-client for \"clickhouse\" catalog spark.sql.catalog.clickhouse.option.async false spark.sql.catalog.clickhouse.option.client_name spark Then you can access ClickHouse table <ck_db>.<ck_table> from Spark SQL by using clickhouse.<ck_db>.<ck_table> .","title":"Single Instance"},{"location":"configurations/01_catalog_configurations/#cluster","text":"For ClickHouse cluster, give an unique catalog name for each instances. Suppose you have two ClickHouse instances, one installed on 10.0.0.1 and exposes gRPC on port 9100 named clickhouse1, and another installed on 10.0.0.2 and exposes gRPC on port 9100 named clickhouse2. Edit $SPARK_HOME/conf/spark-defaults.conf . spark.sql.catalog.clickhouse1 xenon.clickhouse.ClickHouseCatalog spark.sql.catalog.clickhouse1.host 10.0.0.1 spark.sql.catalog.clickhouse1.protocol grpc spark.sql.catalog.clickhouse1.grpc_port 9100 spark.sql.catalog.clickhouse1.user default spark.sql.catalog.clickhouse1.password spark.sql.catalog.clickhouse1.database default spark.sql.catalog.clickhouse1.option.async false spark.sql.catalog.clickhouse2 xenon.clickhouse.ClickHouseCatalog spark.sql.catalog.clickhouse2.host 10.0.0.2 spark.sql.catalog.clickhouse2.protocol grpc spark.sql.catalog.clickhouse2.grpc_port 9100 spark.sql.catalog.clickhouse2.user default spark.sql.catalog.clickhouse2.password spark.sql.catalog.clickhouse2.database default spark.sql.catalog.clickhouse2.option.async false Then you can access clickhouse1 table <ck_db>.<ck_table> from Spark SQL by clickhouse1.<ck_db>.<ck_table> , and access clickhouse2 table <ck_db>.<ck_table> by clickhouse2.<ck_db>.<ck_table> .","title":"Cluster"},{"location":"configurations/02_sql_configurations/","text":"Key Default Description Since spark.clickhouse.ignoreUnsupportedTransform false ClickHouse supports using complex expressions as sharding keys or partition values, e.g. cityHash64(col_1, col_2) , and those can not be supported by Spark now. If true , ignore the unsupported expressions, otherwise fail fast w/ an exception. Note: when spark.clickhouse.write.distributed.convertLocal is enabled, ignore unsupported sharding keys may corrupt the data. 0.4.0 spark.clickhouse.read.compression.codec lz4 The codec used to decompress data for reading. Supported codecs: none, lz4. 0.5.0 spark.clickhouse.read.distributed.convertLocal true When reading Distributed table, read local table instead of itself. If true , ignore spark.clickhouse.read.distributed.useClusterNodes . 0.1.0 spark.clickhouse.read.format json Serialize format for reading. Supported formats: json, binary 0.6.0 spark.clickhouse.read.splitByPartitionId true If true , construct input partition filter by virtual column _partition_id , instead of partition value. There are known bugs to assemble SQL predication by partition value. This feature requires ClickHouse Server v21.6+ 0.4.0 spark.clickhouse.write.batchSize 10000 The number of records per batch on writing to ClickHouse. 0.1.0 spark.clickhouse.write.compression.codec lz4 The codec used to compress data for writing. Supported codecs: none, lz4. 0.3.0 spark.clickhouse.write.distributed.convertLocal false When writing Distributed table, write local table instead of itself. If true , ignore spark.clickhouse.write.distributed.useClusterNodes . 0.1.0 spark.clickhouse.write.distributed.useClusterNodes true Write to all nodes of cluster when writing Distributed table. 0.1.0 spark.clickhouse.write.format arrow Serialize format for writing. Supported formats: json, arrow 0.4.0 spark.clickhouse.write.localSortByKey true If true , do local sort by sort keys before writing. 0.3.0 spark.clickhouse.write.localSortByPartition If true , do local sort by partition before writing. If not set, it equals to spark.clickhouse.write.repartitionByPartition . 0.3.0 spark.clickhouse.write.maxRetry 3 The maximum number of write we will retry for a single batch write failed with retryable codes. 0.1.0 spark.clickhouse.write.repartitionByPartition true Whether to repartition data by ClickHouse partition keys to meet the distributions of ClickHouse table before writing. 0.3.0 spark.clickhouse.write.repartitionNum 0 Repartition data to meet the distributions of ClickHouse table is required before writing, use this conf to specific the repartition number, value less than 1 mean no requirement. 0.1.0 spark.clickhouse.write.repartitionStrictly false If true , Spark will strictly distribute incoming records across partitions to satisfy the required distribution before passing the records to the data source table on write. Otherwise, Spark may apply certain optimizations to speed up the query but break the distribution requirement. Note, this configuration requires SPARK-37523, w/o this patch, it always act as true . 0.3.0 spark.clickhouse.write.retryInterval 10s The interval in seconds between write retry. 0.1.0 spark.clickhouse.write.retryableErrorCodes 241 The retryable error codes returned by ClickHouse server when write failing. 0.1.0","title":"02 sql configurations"},{"location":"developers/","text":"TODO","title":"Index"},{"location":"developers/#todo","text":"","title":"TODO"},{"location":"developers/01_build_and_test/","text":"Build and Test Build Check out source code from GitHub git checkout https://github.com/housepower/spark-clickhouse-connector.git Build w/o test ./gradlew clean build -x test Go to spark-3.3/clickhouse-spark-runtime/build/libs/ to find the output jar clickhouse-spark-runtime-3.3_2.12-0.6.0-SNAPSHOT.jar . Test The project leverage Testcontainers and Docker Compose to do integration tests, you should install Docker and Docker Compose before running test, and check more details on Testcontainers document if you'd like to run test with remote Docker daemon. Run all test ./gradlew clean test Run single test ./gradlew test --tests=ConvertDistToLocalWriteSuite ARM Platform For developers/users who use ARM platform, e.g. Apple Silicon chips, Kunpeng chips, you may not able to run TPC-DS integrations test using gRPC in local directly, because ClickHouse does not provide gRPC support in official ARM image . As a workaround, you can set the environment variable CLICKHOUSE_IMAGE to use a custom image which supports gRPC on ARM platform for testing. export CLICKHOUSE_IMAGE=pan3793/clickhouse-server:22.5.1-alpine-arm-grpc ./gradlew clean test","title":"Build and Test"},{"location":"developers/01_build_and_test/#build-and-test","text":"","title":"Build and Test"},{"location":"developers/01_build_and_test/#build","text":"Check out source code from GitHub git checkout https://github.com/housepower/spark-clickhouse-connector.git Build w/o test ./gradlew clean build -x test Go to spark-3.3/clickhouse-spark-runtime/build/libs/ to find the output jar clickhouse-spark-runtime-3.3_2.12-0.6.0-SNAPSHOT.jar .","title":"Build"},{"location":"developers/01_build_and_test/#test","text":"The project leverage Testcontainers and Docker Compose to do integration tests, you should install Docker and Docker Compose before running test, and check more details on Testcontainers document if you'd like to run test with remote Docker daemon. Run all test ./gradlew clean test Run single test ./gradlew test --tests=ConvertDistToLocalWriteSuite","title":"Test"},{"location":"developers/01_build_and_test/#arm-platform","text":"For developers/users who use ARM platform, e.g. Apple Silicon chips, Kunpeng chips, you may not able to run TPC-DS integrations test using gRPC in local directly, because ClickHouse does not provide gRPC support in official ARM image . As a workaround, you can set the environment variable CLICKHOUSE_IMAGE to use a custom image which supports gRPC on ARM platform for testing. export CLICKHOUSE_IMAGE=pan3793/clickhouse-server:22.5.1-alpine-arm-grpc ./gradlew clean test","title":"ARM Platform"},{"location":"developers/02_docs_and_website/","text":"Docs and Website Setup Python Follow the Python official document to install. Setup pyenv on macOS (optional) Optionally, recommend to manage Python environments by pyenv . Install from Homebrew brew install pyenv pyenv-virtualenv Setup in ~/.zshrc eval \"$(pyenv init -)\" eval \"$(pyenv virtualenv-init -)\" Install virtualenv pyenv install 3.9.13 pyenv virtualenv 3.9.13 scc Localize virtualenv pyenv local scc Install dependencies pip install -r requirements.txt Preview website mkdocs serve Open http://127.0.0.1:8000/ in browser.","title":"Docs and Website"},{"location":"developers/02_docs_and_website/#docs-and-website","text":"","title":"Docs and Website"},{"location":"developers/02_docs_and_website/#setup-python","text":"Follow the Python official document to install.","title":"Setup Python"},{"location":"developers/02_docs_and_website/#setup-pyenv-on-macos-optional","text":"Optionally, recommend to manage Python environments by pyenv . Install from Homebrew brew install pyenv pyenv-virtualenv Setup in ~/.zshrc eval \"$(pyenv init -)\" eval \"$(pyenv virtualenv-init -)\" Install virtualenv pyenv install 3.9.13 pyenv virtualenv 3.9.13 scc Localize virtualenv pyenv local scc","title":"Setup pyenv on macOS (optional)"},{"location":"developers/02_docs_and_website/#install-dependencies","text":"pip install -r requirements.txt","title":"Install dependencies"},{"location":"developers/02_docs_and_website/#preview-website","text":"mkdocs serve Open http://127.0.0.1:8000/ in browser.","title":"Preview website"},{"location":"developers/03_private_release/","text":"Private Release Tip Internal Release means deploying to private Nexus Repository. Please make sure you are granted to access your company private Nexus Repository. Repository and Authentication Configure Gradle in ~/.gradle/gradle.properties . mavenUser=xxx mavenPassword=xxx mavenReleasesRepo=xxx mavenSnapshotsRepo=xxx Upgrade Version Modify version in version.txt and docker/.env-dev Build and Deploy Publish to Maven Repository using ./gradlew publish","title":"Private Release"},{"location":"developers/03_private_release/#private-release","text":"Tip Internal Release means deploying to private Nexus Repository. Please make sure you are granted to access your company private Nexus Repository.","title":"Private Release"},{"location":"developers/03_private_release/#repository-and-authentication","text":"Configure Gradle in ~/.gradle/gradle.properties . mavenUser=xxx mavenPassword=xxx mavenReleasesRepo=xxx mavenSnapshotsRepo=xxx","title":"Repository and Authentication"},{"location":"developers/03_private_release/#upgrade-version","text":"Modify version in version.txt and docker/.env-dev","title":"Upgrade Version"},{"location":"developers/03_private_release/#build-and-deploy","text":"Publish to Maven Repository using ./gradlew publish","title":"Build and Deploy"},{"location":"developers/04_public_release/","text":"Public Release Notice Public Release means deploying to Maven Central. Only core team members are granted to deploy into Public Repository. Note Most of the steps for a public release are done by the GitHub workflow. Snapshot Release The daily snapshot release is managed by Publish Snapshot workflow, it is scheduled to be deployed at midnight every day. Feature Release Cut new branch from master branch, e.g. branch-0.3 ; Update version in version.txt and docker/.env-dev , e.g. from 0.3.0-SNAPSHOT to 0.3.0 ; Create new tag, e.g. v0.3.0 , it will trigger the Publish Release workflow; Verify, close, and release in Sonatype Repository Announce in GitHub Release Update version in version.txt and docker/.env-dev , e.g. from 0.3.0 to 0.3.1-SNAPSHOT ; Update version on master branch in version.txt and docker/.env-dev , e.g. from 0.3.0-SNAPSHOT to 0.4.0-SNAPSHOT ; Publish Docker image after jars available in Maven Central, generally it costs few minutes after step 3. Patch Release Just emit step 1 and step 7 from feature release.","title":"Public Release"},{"location":"developers/04_public_release/#public-release","text":"Notice Public Release means deploying to Maven Central. Only core team members are granted to deploy into Public Repository. Note Most of the steps for a public release are done by the GitHub workflow.","title":"Public Release"},{"location":"developers/04_public_release/#snapshot-release","text":"The daily snapshot release is managed by Publish Snapshot workflow, it is scheduled to be deployed at midnight every day.","title":"Snapshot Release"},{"location":"developers/04_public_release/#feature-release","text":"Cut new branch from master branch, e.g. branch-0.3 ; Update version in version.txt and docker/.env-dev , e.g. from 0.3.0-SNAPSHOT to 0.3.0 ; Create new tag, e.g. v0.3.0 , it will trigger the Publish Release workflow; Verify, close, and release in Sonatype Repository Announce in GitHub Release Update version in version.txt and docker/.env-dev , e.g. from 0.3.0 to 0.3.1-SNAPSHOT ; Update version on master branch in version.txt and docker/.env-dev , e.g. from 0.3.0-SNAPSHOT to 0.4.0-SNAPSHOT ; Publish Docker image after jars available in Maven Central, generally it costs few minutes after step 3.","title":"Feature Release"},{"location":"developers/04_public_release/#patch-release","text":"Just emit step 1 and step 7 from feature release.","title":"Patch Release"},{"location":"internals/","text":"Overview Design In high level, Spark ClickHouse Connector is a connector build on top of Spark DataSource V2 and ClickHouse gRPC protocol.","title":"Index"},{"location":"internals/#overview-design","text":"In high level, Spark ClickHouse Connector is a connector build on top of Spark DataSource V2 and ClickHouse gRPC protocol.","title":"Overview Design"},{"location":"internals/01_catalog/","text":"Catalog Management One important end user facing feature of DataSource V2 is supporting of multi-catalogs. In the early stage of Spark, it does have catalog concept, usually, user use Hive Metastore or Glue to manage table metadata, hence user must register external DataSource tables in centralized metastore before using it. In the centralized metastore model, a table is identified by <database>.<table> . For example, we can register a MySQL table into metastore, then access it using Spark SQL. CREATE TABLE <db>.<tbl> USING org.apache.spark.sql.jdbc OPTIONS ( url \"jdbc:mysql://<mysql_host>:<mysql_port>\", dbtable \"<mysql_db>.<mysql_tbl>\", user \"<mysql_username>\", password \"<mysql_password>\" ); SELECT * FROM <db>.<tbl>; INSERT INTO <db>.<tbl> SELECT ... Things changed in DataSource V2, starting from Spark 3.0, catalog concept is introduced to allow Spark discovering tables automatically by registered catalog plugins. The default catalog has a fixed name spark_catalog , and typically, a table is identified by <catalog>.<database>.<table> . For example, we can register a PostgreSQL database as Spark catalog named pg , and access it using Spark SQL. # spark-defaults.conf spark.sql.catalog.pg=org.apache.spark.sql.execution.datasources.v2.jdbc.JDBCTableCatalog spark.sql.catalog.pg.url=jdbc:postgresql://<pg_host>:<pg_host>/<pg_db> spark.sql.catalog.pg.driver=org.postgresql.Driver spark.sql.catalog.pg.user=<pg_username> spark.sql.catalog.pg.password=<pg_password> SELECT * FROM pg.<db>.<tbl>; INSERT INTO pg.<db>.<tbl> SELECT ...","title":"Catalog"},{"location":"internals/01_catalog/#catalog-management","text":"One important end user facing feature of DataSource V2 is supporting of multi-catalogs. In the early stage of Spark, it does have catalog concept, usually, user use Hive Metastore or Glue to manage table metadata, hence user must register external DataSource tables in centralized metastore before using it. In the centralized metastore model, a table is identified by <database>.<table> . For example, we can register a MySQL table into metastore, then access it using Spark SQL. CREATE TABLE <db>.<tbl> USING org.apache.spark.sql.jdbc OPTIONS ( url \"jdbc:mysql://<mysql_host>:<mysql_port>\", dbtable \"<mysql_db>.<mysql_tbl>\", user \"<mysql_username>\", password \"<mysql_password>\" ); SELECT * FROM <db>.<tbl>; INSERT INTO <db>.<tbl> SELECT ... Things changed in DataSource V2, starting from Spark 3.0, catalog concept is introduced to allow Spark discovering tables automatically by registered catalog plugins. The default catalog has a fixed name spark_catalog , and typically, a table is identified by <catalog>.<database>.<table> . For example, we can register a PostgreSQL database as Spark catalog named pg , and access it using Spark SQL. # spark-defaults.conf spark.sql.catalog.pg=org.apache.spark.sql.execution.datasources.v2.jdbc.JDBCTableCatalog spark.sql.catalog.pg.url=jdbc:postgresql://<pg_host>:<pg_host>/<pg_db> spark.sql.catalog.pg.driver=org.postgresql.Driver spark.sql.catalog.pg.user=<pg_username> spark.sql.catalog.pg.password=<pg_password> SELECT * FROM pg.<db>.<tbl>; INSERT INTO pg.<db>.<tbl> SELECT ...","title":"Catalog Management"},{"location":"internals/02_read/","text":"How reading of the connector works? Push Down Spark supports push down the processing of queries, or parts of queries, into the connected data source. This means that a specific predicate, aggregation function, or other operation, could be passed through to ClickHouse for processing. The results of this push down can include the following benefits: Improved overall query performance Reduced network traffic between Spark and ClickHouse Reduced load on ClickHouse These benefits often result in significant cost reduction. The connector implements most push down interfaces defined by DataSource V2, such as SupportsPushDownLimit , SupportsPushDownFilters , SupportsPushDownAggregates , SupportsPushDownRequiredColumns . The below example shows how SupportsPushDownAggregates and SupportsPushDownRequiredColumns work. Push Down disabled Push Down enabled Bucket Join Sort merge join is a general solution for two large table inner join, it requires two table shuffle by join key first, then do local sort by join key in each data partition, finally do stream-stream like look up to get the final result. In some cases, the tables store collocated by join keys, w/ Storage-Partitioned Join (or V2 Bucket Join), Spark could leverage the existing ClickHouse table layout to eliminate the expensive shuffle and sort operations. Sort Merge Join Bucket Join","title":"Reading"},{"location":"internals/02_read/#how-reading-of-the-connector-works","text":"","title":"How reading of the connector works?"},{"location":"internals/02_read/#push-down","text":"Spark supports push down the processing of queries, or parts of queries, into the connected data source. This means that a specific predicate, aggregation function, or other operation, could be passed through to ClickHouse for processing. The results of this push down can include the following benefits: Improved overall query performance Reduced network traffic between Spark and ClickHouse Reduced load on ClickHouse These benefits often result in significant cost reduction. The connector implements most push down interfaces defined by DataSource V2, such as SupportsPushDownLimit , SupportsPushDownFilters , SupportsPushDownAggregates , SupportsPushDownRequiredColumns . The below example shows how SupportsPushDownAggregates and SupportsPushDownRequiredColumns work. Push Down disabled Push Down enabled","title":"Push Down"},{"location":"internals/02_read/#bucket-join","text":"Sort merge join is a general solution for two large table inner join, it requires two table shuffle by join key first, then do local sort by join key in each data partition, finally do stream-stream like look up to get the final result. In some cases, the tables store collocated by join keys, w/ Storage-Partitioned Join (or V2 Bucket Join), Spark could leverage the existing ClickHouse table layout to eliminate the expensive shuffle and sort operations. Sort Merge Join Bucket Join","title":"Bucket Join"},{"location":"internals/03_write/","text":"How writing of the connector works? As we know, the ClickHouse MergeTree is a LSM-like format, it's not optimized for frequent and random record insertion, batch append operation is recommended for large amount of data ingestion. So, to achieve better performance, we should re-organize the DataFrame to fit ClickHouse data layout before inserting. SPARK-23889 allows data source connector to expose sorting and clustering requirements of DataFrame before writing. By default, for Distributed table, this connector requires the DataFrame clustered by [sharding keys, partition keys] and sorted by [sharding keys, partition keys, ordering keys] ; for normal *MergeTree table, this connector requires the DataFrame sorted by [partition keys, ordering keys] and sorted by [partition keys, ordering keys] . Warning Limitation: Spark does NOT support expressions in sharding keys and partition keys w/o SPARK-39607 . In some cases, the strict data distribution requirements may lead small parallelism and data skew, and finally cause bad performance. SPARK-37523 (requires Spark 3.4+) is introduced to allow relaxing the data distribution requirements to overcome those shortages. Also, you can consider disabling some configurations like spark.clickhouse.write.repartitionByPartition to avoid such performance degradation.","title":"Writing"},{"location":"internals/03_write/#how-writing-of-the-connector-works","text":"As we know, the ClickHouse MergeTree is a LSM-like format, it's not optimized for frequent and random record insertion, batch append operation is recommended for large amount of data ingestion. So, to achieve better performance, we should re-organize the DataFrame to fit ClickHouse data layout before inserting. SPARK-23889 allows data source connector to expose sorting and clustering requirements of DataFrame before writing. By default, for Distributed table, this connector requires the DataFrame clustered by [sharding keys, partition keys] and sorted by [sharding keys, partition keys, ordering keys] ; for normal *MergeTree table, this connector requires the DataFrame sorted by [partition keys, ordering keys] and sorted by [partition keys, ordering keys] . Warning Limitation: Spark does NOT support expressions in sharding keys and partition keys w/o SPARK-39607 . In some cases, the strict data distribution requirements may lead small parallelism and data skew, and finally cause bad performance. SPARK-37523 (requires Spark 3.4+) is introduced to allow relaxing the data distribution requirements to overcome those shortages. Also, you can consider disabling some configurations like spark.clickhouse.write.repartitionByPartition to avoid such performance degradation.","title":"How writing of the connector works?"},{"location":"quick_start/01_get_the_library/","text":"Get the Library Download the Library The name pattern of binary jar is clickhouse-spark-runtime-${spark_binary_version}_${scala_binary_version}-${version}.jar you can find all available released jars under Maven Central Repository and all daily build SNAPSHOT jars under Sonatype OSS Snapshots Repository . Import as Dependency Gradle dependencies { implementation(\"com.github.housepower:clickhouse-spark-runtime-3.3_2.12:0.5.0\") implementation(\"com.clickhouse:clickhouse-jdbc:0.3.2-patch11:all\") { transitive = false } } Add the following repository if you want to use SNAPSHOT version. repositries { maven { url = \"https://oss.sonatype.org/content/repositories/snapshots\" } } Maven <dependency> <groupId>com.github.housepower</groupId> <artifactId>clickhouse-spark-runtime-3.3_2.12</artifactId> <version>0.5.0</version> </dependency> <dependency> <groupId>com.clickhouse</groupId> <artifactId>clickhouse-jdbc</artifactId> <classifier>all</classifier> <version>0.3.2-patch11</version> <exclusions> <exclusion> <groupId>*</groupId> <artifactId>*</artifactId> </exclusion> </exclusions> </dependency> Add the following repository if you want to use SNAPSHOT version. <repositories> <repository> <id>sonatype-oss-snapshots</id> <name>Sonatype OSS Snapshots Repository</name> <url>https://oss.sonatype.org/content/repositories/snapshots</url> </repository> </repositories>","title":"Get the Library"},{"location":"quick_start/01_get_the_library/#get-the-library","text":"","title":"Get the Library"},{"location":"quick_start/01_get_the_library/#download-the-library","text":"The name pattern of binary jar is clickhouse-spark-runtime-${spark_binary_version}_${scala_binary_version}-${version}.jar you can find all available released jars under Maven Central Repository and all daily build SNAPSHOT jars under Sonatype OSS Snapshots Repository .","title":"Download the Library"},{"location":"quick_start/01_get_the_library/#import-as-dependency","text":"","title":"Import as Dependency"},{"location":"quick_start/01_get_the_library/#gradle","text":"dependencies { implementation(\"com.github.housepower:clickhouse-spark-runtime-3.3_2.12:0.5.0\") implementation(\"com.clickhouse:clickhouse-jdbc:0.3.2-patch11:all\") { transitive = false } } Add the following repository if you want to use SNAPSHOT version. repositries { maven { url = \"https://oss.sonatype.org/content/repositories/snapshots\" } }","title":"Gradle"},{"location":"quick_start/01_get_the_library/#maven","text":"<dependency> <groupId>com.github.housepower</groupId> <artifactId>clickhouse-spark-runtime-3.3_2.12</artifactId> <version>0.5.0</version> </dependency> <dependency> <groupId>com.clickhouse</groupId> <artifactId>clickhouse-jdbc</artifactId> <classifier>all</classifier> <version>0.3.2-patch11</version> <exclusions> <exclusion> <groupId>*</groupId> <artifactId>*</artifactId> </exclusion> </exclusions> </dependency> Add the following repository if you want to use SNAPSHOT version. <repositories> <repository> <id>sonatype-oss-snapshots</id> <name>Sonatype OSS Snapshots Repository</name> <url>https://oss.sonatype.org/content/repositories/snapshots</url> </repository> </repositories>","title":"Maven"},{"location":"quick_start/02_play_with_spark_sql/","text":"Play with Spark SQL Note: For SQL-only use cases, Apache Kyuubi is recommended for Production. Launch Spark SQL CLI $SPARK_HOME/bin/spark-sql \\ --conf spark.sql.catalog.clickhouse=xenon.clickhouse.ClickHouseCatalog \\ --conf spark.sql.catalog.clickhouse.host=${CLICKHOUSE_HOST:-127.0.0.1} \\ --conf spark.sql.catalog.clickhouse.protocol=http \\ --conf spark.sql.catalog.clickhouse.http_port=${CLICKHOUSE_GRPC_PORT:-8123} \\ --conf spark.sql.catalog.clickhouse.user=${CLICKHOUSE_USER:-default} \\ --conf spark.sql.catalog.clickhouse.password=${CLICKHOUSE_PASSWORD:-} \\ --conf spark.sql.catalog.clickhouse.database=default \\ --jars /path/clickhouse-spark-runtime-3.3_2.12:0.5.0.jar,/path/clickhouse-jdbc-0.3.2-patch11-all.jar The following argument --jars /path/clickhouse-spark-runtime-3.3_2.12:0.5.0.jar,/path/clickhouse-jdbc-0.3.2-patch11-all.jar can be replaced by --repositories https://{maven-cental-mirror or private-nexus-repo} \\ --packages com.github.housepower:clickhouse-spark-runtime-3.3_2.12:0.5.0,com.clickhouse:clickhouse-jdbc:0.3.2-patch11:all to avoid copying jar to your Spark client node. Operations Basic operations, e.g. create database, create table, write table, read table, etc. spark-sql> use clickhouse; Time taken: 0.016 seconds spark-sql> create database if not exists test_db; Time taken: 0.022 seconds spark-sql> show databases; default system test_db Time taken: 0.289 seconds, Fetched 3 row(s) spark-sql> CREATE TABLE test_db.tbl_sql ( > create_time TIMESTAMP NOT NULL, > m INT NOT NULL COMMENT 'part key', > id BIGINT NOT NULL COMMENT 'sort key', > value STRING > ) USING ClickHouse > PARTITIONED BY (m) > TBLPROPERTIES ( > engine = 'MergeTree()', > order_by = 'id', > settings.index_granularity = 8192 > ); Time taken: 0.242 seconds spark-sql> insert into test_db.tbl_sql values > (timestamp'2021-01-01 10:10:10', 1, 1L, '1'), > (timestamp'2022-02-02 10:10:10', 2, 2L, '2') > as tabl(create_time, m, id, value); Time taken: 0.276 seconds spark-sql> select * from test_db.tbl_sql; 2021-01-01 10:10:10 1 1 1 2022-02-02 10:10:10 2 2 2 Time taken: 0.116 seconds, Fetched 2 row(s) spark-sql> insert into test_db.tbl_sql select * from test_db.tbl_sql; Time taken: 1.028 seconds spark-sql> insert into test_db.tbl_sql select * from test_db.tbl_sql; Time taken: 0.462 seconds spark-sql> select count(*) from test_db.tbl_sql; 6 Time taken: 1.421 seconds, Fetched 1 row(s) spark-sql> select * from test_db.tbl_sql; 2021-01-01 10:10:10 1 1 1 2021-01-01 10:10:10 1 1 1 2021-01-01 10:10:10 1 1 1 2022-02-02 10:10:10 2 2 2 2022-02-02 10:10:10 2 2 2 2022-02-02 10:10:10 2 2 2 Time taken: 0.123 seconds, Fetched 6 row(s)","title":"Play with Spark SQL"},{"location":"quick_start/02_play_with_spark_sql/#play-with-spark-sql","text":"Note: For SQL-only use cases, Apache Kyuubi is recommended for Production.","title":"Play with Spark SQL"},{"location":"quick_start/02_play_with_spark_sql/#launch-spark-sql-cli","text":"$SPARK_HOME/bin/spark-sql \\ --conf spark.sql.catalog.clickhouse=xenon.clickhouse.ClickHouseCatalog \\ --conf spark.sql.catalog.clickhouse.host=${CLICKHOUSE_HOST:-127.0.0.1} \\ --conf spark.sql.catalog.clickhouse.protocol=http \\ --conf spark.sql.catalog.clickhouse.http_port=${CLICKHOUSE_GRPC_PORT:-8123} \\ --conf spark.sql.catalog.clickhouse.user=${CLICKHOUSE_USER:-default} \\ --conf spark.sql.catalog.clickhouse.password=${CLICKHOUSE_PASSWORD:-} \\ --conf spark.sql.catalog.clickhouse.database=default \\ --jars /path/clickhouse-spark-runtime-3.3_2.12:0.5.0.jar,/path/clickhouse-jdbc-0.3.2-patch11-all.jar The following argument --jars /path/clickhouse-spark-runtime-3.3_2.12:0.5.0.jar,/path/clickhouse-jdbc-0.3.2-patch11-all.jar can be replaced by --repositories https://{maven-cental-mirror or private-nexus-repo} \\ --packages com.github.housepower:clickhouse-spark-runtime-3.3_2.12:0.5.0,com.clickhouse:clickhouse-jdbc:0.3.2-patch11:all to avoid copying jar to your Spark client node.","title":"Launch Spark SQL CLI"},{"location":"quick_start/02_play_with_spark_sql/#operations","text":"Basic operations, e.g. create database, create table, write table, read table, etc. spark-sql> use clickhouse; Time taken: 0.016 seconds spark-sql> create database if not exists test_db; Time taken: 0.022 seconds spark-sql> show databases; default system test_db Time taken: 0.289 seconds, Fetched 3 row(s) spark-sql> CREATE TABLE test_db.tbl_sql ( > create_time TIMESTAMP NOT NULL, > m INT NOT NULL COMMENT 'part key', > id BIGINT NOT NULL COMMENT 'sort key', > value STRING > ) USING ClickHouse > PARTITIONED BY (m) > TBLPROPERTIES ( > engine = 'MergeTree()', > order_by = 'id', > settings.index_granularity = 8192 > ); Time taken: 0.242 seconds spark-sql> insert into test_db.tbl_sql values > (timestamp'2021-01-01 10:10:10', 1, 1L, '1'), > (timestamp'2022-02-02 10:10:10', 2, 2L, '2') > as tabl(create_time, m, id, value); Time taken: 0.276 seconds spark-sql> select * from test_db.tbl_sql; 2021-01-01 10:10:10 1 1 1 2022-02-02 10:10:10 2 2 2 Time taken: 0.116 seconds, Fetched 2 row(s) spark-sql> insert into test_db.tbl_sql select * from test_db.tbl_sql; Time taken: 1.028 seconds spark-sql> insert into test_db.tbl_sql select * from test_db.tbl_sql; Time taken: 0.462 seconds spark-sql> select count(*) from test_db.tbl_sql; 6 Time taken: 1.421 seconds, Fetched 1 row(s) spark-sql> select * from test_db.tbl_sql; 2021-01-01 10:10:10 1 1 1 2021-01-01 10:10:10 1 1 1 2021-01-01 10:10:10 1 1 1 2022-02-02 10:10:10 2 2 2 2022-02-02 10:10:10 2 2 2 2022-02-02 10:10:10 2 2 2 Time taken: 0.123 seconds, Fetched 6 row(s)","title":"Operations"},{"location":"quick_start/03_play_with_spark_shell/","text":"Play with Spark Shell Launch Spark Shell $SPARK_HOME/bin/spark-shell \\ --conf spark.sql.catalog.clickhouse=xenon.clickhouse.ClickHouseCatalog \\ --conf spark.sql.catalog.clickhouse.host=${CLICKHOUSE_HOST:-127.0.0.1} \\ --conf spark.sql.catalog.clickhouse.protocol=grpc \\ --conf spark.sql.catalog.clickhouse.grpc_port=${CLICKHOUSE_GRPC_PORT:-9100} \\ --conf spark.sql.catalog.clickhouse.user=${CLICKHOUSE_USER:-default} \\ --conf spark.sql.catalog.clickhouse.password=${CLICKHOUSE_PASSWORD:-} \\ --conf spark.sql.catalog.clickhouse.database=default \\ --jars /path/clickhouse-spark-runtime-3.3_2.12:0.5.0.jar,/path/clickhouse-jdbc-0.3.2-patch11-all.jar The following argument --jars /path/clickhouse-spark-runtime-3.3_2.12:0.5.0.jar,/path/clickhouse-jdbc-0.3.2-patch11-all.jar can be replaced by --repositories https://{maven-cental-mirror or private-nexus-repo} \\ --packages com.github.housepower:clickhouse-spark-runtime-3.3_2.12:0.5.0,com.clickhouse:clickhouse-jdbc:0.3.2-patch11:all to avoid copying jar to your Spark client node. Operations Basic operations, e.g. create database, create table, write table, read table, etc. scala> spark.sql(\"use clickhouse\") res0: org.apache.spark.sql.DataFrame = [] scala> spark.sql(\"create database test_db\") res1: org.apache.spark.sql.DataFrame = [] scala> spark.sql(\"show databases\").show +---------+ |namespace| +---------+ | default| | system| | test_db| +---------+ scala> spark.sql(\"\"\" | CREATE TABLE test_db.tbl ( | create_time TIMESTAMP NOT NULL, | m INT NOT NULL COMMENT 'part key', | id BIGINT NOT NULL COMMENT 'sort key', | value STRING | ) USING ClickHouse | PARTITIONED BY (m) | TBLPROPERTIES ( | engine = 'MergeTree()', | order_by = 'id', | settings.index_granularity = 8192 | ) | \"\"\") res2: org.apache.spark.sql.DataFrame = [] scala> :paste // Entering paste mode (ctrl-D to finish) spark.createDataFrame(Seq( (\"2021-01-01 10:10:10\", 1L, \"1\"), (\"2022-02-02 10:10:10\", 2L, \"2\") )).toDF(\"create_time\", \"id\", \"value\") .withColumn(\"create_time\", to_timestamp($\"create_time\")) .withColumn(\"m\", month($\"create_time\")) .select($\"create_time\", $\"m\", $\"id\", $\"value\") .writeTo(\"test_db.tbl\") .append // Exiting paste mode, now interpreting. scala> spark.table(\"test_db.tbl\").show +-------------------+---+---+-----+ | create_time| m| id|value| +-------------------+---+---+-----+ |2021-01-01 10:10:10| 1| 1| 1| |2022-02-02 10:10:10| 2| 2| 2| +-------------------+---+---+-----+ Execute ClickHouse native SQL. scala> val options = Map( | \"host\" -> \"clickhouse\", | \"protocol\" -> \"grpc\", | \"grpc_port\" -> \"9100\", | \"user\" -> \"default\", | \"password\" -> \"\" | ) scala> val sql = \"\"\" | |CREATE TABLE test_db.person ( | | id Int64, | | name String, | | age Nullable(Int32) | |) | |ENGINE = MergeTree() | |ORDER BY id | \"\"\".stripMargin scala> spark.executeCommand(\"xenon.clickhouse.ClickHouseCommandRunner\", sql, options) scala> spark.sql(\"show tables in clickhouse_s1r1.test_db\").show +---------+---------+-----------+ |namespace|tableName|isTemporary| +---------+---------+-----------+ | test_db| person| false| +---------+---------+-----------+ scala> spark.table(\"clickhouse_s1r1.test_db.person\").printSchema root |-- id: long (nullable = false) |-- name: string (nullable = false) |-- age: integer (nullable = true)","title":"Play with Spark Shell"},{"location":"quick_start/03_play_with_spark_shell/#play-with-spark-shell","text":"","title":"Play with Spark Shell"},{"location":"quick_start/03_play_with_spark_shell/#launch-spark-shell","text":"$SPARK_HOME/bin/spark-shell \\ --conf spark.sql.catalog.clickhouse=xenon.clickhouse.ClickHouseCatalog \\ --conf spark.sql.catalog.clickhouse.host=${CLICKHOUSE_HOST:-127.0.0.1} \\ --conf spark.sql.catalog.clickhouse.protocol=grpc \\ --conf spark.sql.catalog.clickhouse.grpc_port=${CLICKHOUSE_GRPC_PORT:-9100} \\ --conf spark.sql.catalog.clickhouse.user=${CLICKHOUSE_USER:-default} \\ --conf spark.sql.catalog.clickhouse.password=${CLICKHOUSE_PASSWORD:-} \\ --conf spark.sql.catalog.clickhouse.database=default \\ --jars /path/clickhouse-spark-runtime-3.3_2.12:0.5.0.jar,/path/clickhouse-jdbc-0.3.2-patch11-all.jar The following argument --jars /path/clickhouse-spark-runtime-3.3_2.12:0.5.0.jar,/path/clickhouse-jdbc-0.3.2-patch11-all.jar can be replaced by --repositories https://{maven-cental-mirror or private-nexus-repo} \\ --packages com.github.housepower:clickhouse-spark-runtime-3.3_2.12:0.5.0,com.clickhouse:clickhouse-jdbc:0.3.2-patch11:all to avoid copying jar to your Spark client node.","title":"Launch Spark Shell"},{"location":"quick_start/03_play_with_spark_shell/#operations","text":"Basic operations, e.g. create database, create table, write table, read table, etc. scala> spark.sql(\"use clickhouse\") res0: org.apache.spark.sql.DataFrame = [] scala> spark.sql(\"create database test_db\") res1: org.apache.spark.sql.DataFrame = [] scala> spark.sql(\"show databases\").show +---------+ |namespace| +---------+ | default| | system| | test_db| +---------+ scala> spark.sql(\"\"\" | CREATE TABLE test_db.tbl ( | create_time TIMESTAMP NOT NULL, | m INT NOT NULL COMMENT 'part key', | id BIGINT NOT NULL COMMENT 'sort key', | value STRING | ) USING ClickHouse | PARTITIONED BY (m) | TBLPROPERTIES ( | engine = 'MergeTree()', | order_by = 'id', | settings.index_granularity = 8192 | ) | \"\"\") res2: org.apache.spark.sql.DataFrame = [] scala> :paste // Entering paste mode (ctrl-D to finish) spark.createDataFrame(Seq( (\"2021-01-01 10:10:10\", 1L, \"1\"), (\"2022-02-02 10:10:10\", 2L, \"2\") )).toDF(\"create_time\", \"id\", \"value\") .withColumn(\"create_time\", to_timestamp($\"create_time\")) .withColumn(\"m\", month($\"create_time\")) .select($\"create_time\", $\"m\", $\"id\", $\"value\") .writeTo(\"test_db.tbl\") .append // Exiting paste mode, now interpreting. scala> spark.table(\"test_db.tbl\").show +-------------------+---+---+-----+ | create_time| m| id|value| +-------------------+---+---+-----+ |2021-01-01 10:10:10| 1| 1| 1| |2022-02-02 10:10:10| 2| 2| 2| +-------------------+---+---+-----+ Execute ClickHouse native SQL. scala> val options = Map( | \"host\" -> \"clickhouse\", | \"protocol\" -> \"grpc\", | \"grpc_port\" -> \"9100\", | \"user\" -> \"default\", | \"password\" -> \"\" | ) scala> val sql = \"\"\" | |CREATE TABLE test_db.person ( | | id Int64, | | name String, | | age Nullable(Int32) | |) | |ENGINE = MergeTree() | |ORDER BY id | \"\"\".stripMargin scala> spark.executeCommand(\"xenon.clickhouse.ClickHouseCommandRunner\", sql, options) scala> spark.sql(\"show tables in clickhouse_s1r1.test_db\").show +---------+---------+-----------+ |namespace|tableName|isTemporary| +---------+---------+-----------+ | test_db| person| false| +---------+---------+-----------+ scala> spark.table(\"clickhouse_s1r1.test_db.person\").printSchema root |-- id: long (nullable = false) |-- name: string (nullable = false) |-- age: integer (nullable = true)","title":"Operations"}]}